{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Testing OPE with trained Algo"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "285d30464e1888cf"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-11T09:45:00.100028Z",
     "start_time": "2023-12-11T09:45:00.093317Z"
    }
   },
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.rllib.offline import JsonReader\n",
    "from ray.rllib.offline.estimators import WeightedImportanceSampling\n",
    "\n",
    "from typing import (\n",
    "    TYPE_CHECKING,\n",
    "    Any,\n",
    "    Callable,\n",
    "    Container,\n",
    "    Dict,\n",
    "    List,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Tuple,\n",
    "    Type,\n",
    "    Union,\n",
    ")\n",
    "from ray.rllib.utils.typing import (\n",
    "    TensorType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "algo_path = '/Users/jk1/temp/ope_tests/custom_data_out/crr_model'\n",
    "data_path = '/Users/jk1/temp/ope_tests/custom_data_out/output-2023-12-10_21-01-48_worker-0_0.json'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T08:37:26.878316Z",
     "start_time": "2023-12-11T08:37:26.866005Z"
    }
   },
   "id": "2f5f46130af5e91c"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 09:27:16,717\tWARNING __init__.py:10 -- CRR has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\n",
      "2023-12-11 09:27:16,722\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/crr/` has been deprecated. Use `rllib_contrib/crr/` instead. This will raise an error in the future!\n",
      "/Users/jk1/opt/anaconda3/envs/cats3/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/Users/jk1/opt/anaconda3/envs/cats3/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/jk1/opt/anaconda3/envs/cats3/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/jk1/opt/anaconda3/envs/cats3/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2023-12-11 09:27:16,811\tWARNING __init__.py:10 -- DDPG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\n",
      "2023-12-11 09:27:19,328\tINFO worker.py:1673 -- Started a local Ray instance.\n",
      "\u001B[33m(raylet)\u001B[0m [2023-12-11 09:27:29,318 E 9261 301110] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-12-11_09-27-16_893782_9230 is over 95% full, available space: 4492726272; capacity: 499963174912. Object creation will fail if spilling is required.\n",
      "\u001B[36m(RolloutWorker pid=9284)\u001B[0m 2023-12-11 09:27:35,407\tWARNING __init__.py:10 -- CRR has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\n",
      "\u001B[36m(RolloutWorker pid=9284)\u001B[0m 2023-12-11 09:27:35,424\tWARNING __init__.py:10 -- DDPG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\n",
      "2023-12-11 09:27:36,556\tINFO trainable.py:164 -- Trainable.setup took 19.781 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-12-11 09:27:36,558\tWARNING util.py:62 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "trained_algo = Algorithm.from_checkpoint(algo_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T08:27:36.626841Z",
     "start_time": "2023-12-11T08:27:16.716641Z"
    }
   },
   "id": "550cfd23879ef20e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "CRRTorchPolicy"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_policy = trained_algo.get_policy()\n",
    "algo_policy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T08:27:44.778087Z",
     "start_time": "2023-12-11T08:27:44.771358Z"
    }
   },
   "id": "764ba01c4f941b4c"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WeightedImportanceSampling.__init__() missing 1 required positional argument: 'policy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m estimator \u001B[38;5;241m=\u001B[39m \u001B[43mWeightedImportanceSampling\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# policy=algo_policy,\u001B[39;49;00m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgamma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.99\u001B[39;49m\n\u001B[1;32m      4\u001B[0m \u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: WeightedImportanceSampling.__init__() missing 1 required positional argument: 'policy'"
     ]
    }
   ],
   "source": [
    "estimator = WeightedImportanceSampling(\n",
    "    policy=algo_policy,\n",
    "    gamma=0.99\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T08:42:49.405923Z",
     "start_time": "2023-12-11T08:42:49.366897Z"
    }
   },
   "id": "ef3233b2b0a7f379"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33m(raylet)\u001B[0m [2023-12-11 09:38:43,410 E 9261 301110] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-12-11_09-27-16_893782_9230 is over 95% full, available space: 4418088960; capacity: 499963174912. Object creation will fail if spilling is required.\n"
     ]
    }
   ],
   "source": [
    "reader = JsonReader(data_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T08:38:43.659611Z",
     "start_time": "2023-12-11T08:38:43.644165Z"
    }
   },
   "id": "25b1881f5135b096"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- v_behavior: The discounted return averaged over episodes in the batch\n",
    "- v_behavior_std: The standard deviation corresponding to v_behavior\n",
    "- v_target: The estimated discounted return for `self.policy`,\n",
    "averaged over episodes in the batch\n",
    "- v_target_std: The standard deviation corresponding to v_target\n",
    "- v_gain: v_target / max(v_behavior, 1e-8)\n",
    "- v_delta: The difference between v_target and v_behavior."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3dde55c18847598"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'v_behavior': 0.0, 'v_behavior_std': 0.0, 'v_target': 0.0, 'v_target_std': 0.0, 'v_gain': 0.0, 'v_delta': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Compute off-policy estimates\n",
    "for _ in range(2):\n",
    "    batch = reader.next()\n",
    "    print(estimator.estimate(batch))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T09:30:53.541644Z",
     "start_time": "2023-12-11T09:30:53.507644Z"
    }
   },
   "id": "53be33e77ee13803"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a custom policy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17ed2a992412be92"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Custom constant policy\n",
    "- Always return 7 (not limited)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97aba22451030f72"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from ray.rllib import Policy\n",
    "\n",
    "\n",
    "class CustomPolicy(Policy):\n",
    "    \"\"\"Example of a custom policy always returning a single action - 7\n",
    "\n",
    "    You might find it more convenient to use the `build_tf_policy` and\n",
    "    `build_torch_policy` helpers instead for a real policy, which are\n",
    "    described in the next sections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space, config):\n",
    "        Policy.__init__(self, observation_space, action_space, config)\n",
    "        # example parameter\n",
    "        self.w = 1.0\n",
    "\n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        info_batch=None,\n",
    "                        episodes=None,\n",
    "                        **kwargs):\n",
    "        # return action batch, RNN states, extra values to include in batch\n",
    "        return [7 for _ in obs_batch], [], {}\n",
    "\n",
    "    def compute_log_likelihoods(\n",
    "        self,\n",
    "        actions: Union[List[TensorType], TensorType],\n",
    "        obs_batch: Union[List[TensorType], TensorType],\n",
    "        state_batches: Optional[List[TensorType]] = None,\n",
    "        prev_action_batch: Optional[Union[List[TensorType], TensorType]] = None,\n",
    "        prev_reward_batch: Optional[Union[List[TensorType], TensorType]] = None,\n",
    "        actions_normalized: bool = True,\n",
    "        in_training: bool = True,\n",
    "    ) -> TensorType:\n",
    "        \"\"\"Computes the log-prob/likelihood for a given action and observation.\n",
    "\n",
    "        The log-likelihood is calculated using this Policy's action\n",
    "        distribution class (self.dist_class).\n",
    "        \n",
    "        In this example, check if the given action is equal to 7 and return\n",
    "        a log-prob of 0.0 if so, otherwise -inf.\n",
    "\n",
    "        Args:\n",
    "            actions: Batch of actions, for which to retrieve the\n",
    "                log-probs/likelihoods (given all other inputs: obs,\n",
    "                states, ..).\n",
    "            obs_batch: Batch of observations.\n",
    "            state_batches: List of RNN state input batches, if any.\n",
    "            prev_action_batch: Batch of previous action values.\n",
    "            prev_reward_batch: Batch of previous rewards.\n",
    "            actions_normalized: Is the given `actions` already normalized\n",
    "                (between -1.0 and 1.0) or not? If not and\n",
    "                `normalize_actions=True`, we need to normalize the given\n",
    "                actions first, before calculating log likelihoods.\n",
    "            in_training: Whether to use the forward_train() or forward_exploration() of\n",
    "                the underlying RLModule.\n",
    "        Returns:\n",
    "            Batch of log probs/likelihoods, with shape: [BATCH_SIZE].\n",
    "        \"\"\"\n",
    "        \n",
    "        # return log-likelihoods\n",
    "        return [0.0 if a == 7 else float(\"-inf\") for a in actions]\n",
    "        \n",
    "    # def learn_on_batch(self, samples):\n",
    "    #     # implement your learning code here\n",
    "    #     return {}  # return stats\n",
    "    # \n",
    "    # def get_weights(self):\n",
    "    #     return {\"w\": self.w}\n",
    "    # \n",
    "    # def set_weights(self, weights):\n",
    "    #     self.w = weights[\"w\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T09:49:35.357237Z",
     "start_time": "2023-12-11T09:49:35.347402Z"
    }
   },
   "id": "dce0066848c3d1d2"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# register custom policy\n",
    "custom_policy = CustomPolicy(observation_space=None, action_space=None, config={})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T09:49:37.327408Z",
     "start_time": "2023-12-11T09:49:37.322724Z"
    }
   },
   "id": "d1fc77a49c80b966"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], [], {})\n",
      "([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], [], {})\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    batch = reader.next()\n",
    "    print(custom_policy.compute_actions(batch['obs'], []))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T09:49:40.765642Z",
     "start_time": "2023-12-11T09:49:40.757686Z"
    }
   },
   "id": "86485f74e66fab7f"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# evaluate custom_policy\n",
    "estimator = WeightedImportanceSampling(\n",
    "    policy=custom_policy,\n",
    "    gamma=0.99\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T09:49:42.497054Z",
     "start_time": "2023-12-11T09:49:42.491260Z"
    }
   },
   "id": "e44b0b1a1c0bc133"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'v_behavior': 0.49483865960020695, 'v_behavior_std': 0.0, 'v_target': 0.49483865960020695, 'v_target_std': 0.0, 'v_gain': 1.0, 'v_delta': 0.0}\n",
      "{'v_behavior': 0.0, 'v_behavior_std': 0.0, 'v_target': nan, 'v_target_std': nan, 'v_gain': nan, 'v_delta': nan}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jk1/opt/anaconda3/envs/cats3/lib/python3.10/site-packages/ray/rllib/offline/estimators/weighted_importance_sampling.py:66: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  v_target += episode_p[t] / w_t * rewards[t] * self.gamma**t\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    batch = reader.next()\n",
    "    print(estimator.estimate(batch))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T09:49:43.508115Z",
     "start_time": "2023-12-11T09:49:43.495745Z"
    }
   },
   "id": "f9efb66f4e634448"
  },
  {
   "cell_type": "markdown",
   "source": [
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de7d48e02194ce02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "43126ddb7e146091"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
